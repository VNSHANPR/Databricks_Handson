{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37d4f84-2097-4e05-80ce-d3c976b4ca89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Fund Flow Analysis : Working Capital Insights\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://www.databricks.com/sites/default/files/styles/max_1000x1000/public/2025-02/sap-launch-og-image-alt-03a.png?itok=JlWcDAln&v=1739438590\" alt=\"Databricks Learning\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",

    "Facing significant **free cash flow challenges**, **ABC International Group** urgently needs to **streamline its working capital**. Key to this is improving **invoice collection**, gaining deeper insights into customer payment behaviors, and accurately predicting payments to enhance the dunning process and improve overall cash flow visibility. \n",
    "\n",
    "This notebook addresses this need by exploring the prediction of **SAP invoice clearing dates**. It details a custom feature engineering and machine learning prediction scenario implemented on data extracted from the **S/4HANA CDS view** for accounting line items within SAP S/4HANA Accounts Receivable scenario.\n",
    " \n",
    " With the Integrated version of **SAP Databricks in SAP Business Data Cloud(BDC)** it may well be possible to utilize an existing **Data Product** and share with the Databricks Envionment using the share option in BDC catalog. This will make the dataset available in Databricks unity catalog for further processing. \n",
    "\n",
    " Using this Notebook you will understand the basics of data processing in Databricks, along with explanation of fundamental features in Databricks like : \n",
    "\n",
    "-  **Delta Lake**\n",
    "-  **Working on a Delta Lake table**\n",
    "-  **Lakehouse Architecture**\n",
    "-  **Unity Catalog** in Databricks\n",
    "-  **Difference between Pandas dataframe on VM/local machines vs using pyspark dataframe on databricks cluster**\n",
    "-  **Pyspark Dataframe** API for Data modelling and feature engineering \n",
    "-  **ML Model training** and batch inference\n",
    "-  **ML Flow** to track ML experiments\n",
    "\n",
    "\n",
    "\n",
    "# **DATASET**\n",
    "-  Dataset can be **extracted from S/4HANA system** using **SAP CDS view : I_OperationalAcctgDocItem** , wherein we will have a \"Clearing Date\" for accounting line item.\n",
    "- Similarly it can also be extracted using a **custom SAP report**.\n",
    "- In future this may well come from a **standard SAP Data product**, with sharing of dataset directly enabled within the SAP Business Data Cloud framework, opening up the Databricks notebook with shared data product inbuilt. \n",
    "\n",
    "\n",
    "\n",
    "        Existing Dataset columns : \n",
    "\n",
    "       'Branch', 'Branch Name', 'Customer Code', 'Customer Name Sold T',\n",
    "       'Document Type', 'Document Number', 'Document Date', 'Due Date',\n",
    "       'Document Amount', 'Outstanding Amount', 'Business', 'Payment Terms',\n",
    "       'Transaction Cheque', 'Cover Cheque', 'Overdue', 'TDS', 'Division',\n",
    "       'Customer Reference', 'Group', 'SBU', 'Sub Sbu Desc',\n",
    "       'Business Description', 'Customer Profile', 'Payment Method',\n",
    "       'POD Available', 'Text', 'ECB', 'Bill To Party Code',\n",
    "       'Bill To Party Name', 'Ship to Party Code', 'Ship to Party Name',\n",
    "       'End Customer Details', 'OEM/NOC', 'Note Attachment', 'Clearing Date',\n",
    "       'Plant', 'Plant Description', 'Invoice Ageing(Days)'\n",
    "\n",
    "# **Architecture and Technical Flow**\n",
    "\n",
    "##  **SAP BDC**\n",
    "\n",
    "<br />\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-06 at 11.50.03 AM.png\" alt=\"Alt Text\" width=\"1000\" height=\"1200\">\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2341de30-89eb-42c6-8b72-d68a1f99b7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Requirements - Attach Compute\n",
    " \n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "Please open the notebook in **one additonal tab** to follow : \n",
    "\n",
    "* To run this notebook, **attach your notebook** to the \"**[Unity]XS all_purpose cluster\"**, by navigating to compute section in the left navigation bar!\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-14 at 5.24.12 PM.png\" alt=\"Databricks Learning\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-14 at 5.25.27 PM.png\" alt=\"Databricks Learning\" width=\"500\" height=\"600\" >\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-14 at 5.27.15 PM.png\" alt=\"Databricks Learning\"  >\n",
    "</div>\n",
    "\n",
    "Use **connect** at the top right to attach the notebook to the cluster configured above. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e47be0e-3158-4d46-bd8c-f5239facf850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Load \n",
    "We are generating S/4HANA data for collections of invoices, the data here includes basic details of the Invoices along with the **Clearing Date**. We will use the clearing date to calculate more features like **Days_to_Pay** the invoice etc. This will form as the **label** or key feature to work with for predicting the future payment dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "45f8b608-2a74-4d70-915e-bc29fe22431e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, date\n",
    "num_invoices = 200000\n",
    "num_customers = 25\n",
    "\n",
    "def sample_s4_data (num_invoices,num_customers):\n",
    "    # Number of invoices to generate\n",
    "    num_invoices = num_invoices\n",
    "    num_customers = num_customers\n",
    "\n",
    "    # Generate unique document numbers\n",
    "    document_numbers = [f\"INV-{i+1:06d}\" for i in range(num_invoices)]\n",
    "\n",
    "    # Generate document dates within a reasonable range\n",
    "    start_date = date(2023, 1, 1)\n",
    "    end_date = date(2025, 4, 30)\n",
    "    time_diff = (end_date - start_date).days\n",
    "    document_dates = [start_date + timedelta(days=np.random.randint(time_diff)) for _ in range(num_invoices)]\n",
    "\n",
    "    # Generate random due dates (within 90 days of document date)\n",
    "    due_dates = [doc_date + timedelta(days=np.random.randint(1, 91)) for doc_date in document_dates]\n",
    "\n",
    "    # Generate random document amounts\n",
    "    document_amounts = np.random.uniform(10, 10000, num_invoices).round(2)\n",
    "\n",
    "    # Generate random outstanding amounts (less than or equal to document amount)\n",
    "    outstanding_amounts = (document_amounts * np.random.uniform(0, 1, num_invoices)).round(2)\n",
    "\n",
    "    # Generate clearing dates (0 to 500 days after document date)\n",
    "    clearing_dates = [doc_date + timedelta(days=np.random.normal(58,15)) for doc_date in document_dates]\n",
    "\n",
    "    # Generate random data for other columns\n",
    "    branches = np.random.choice([\"North\", \"South\", \"East\", \"West\", \"Central\"], num_invoices)\n",
    "    branch_names = np.random.choice([\"North Branch\", \"South Branch\", \"East Branch\", \"West Branch\", \"Central Branch\"], num_invoices)\n",
    "    customer_codes = [f\"CUST{i:04d}\" for i in np.random.choice(range(1, num_customers + 1), num_invoices)]\n",
    "    customer_names = [f\"Customer {i}\" for i in np.random.choice(range(1, num_customers + 1), num_invoices)]\n",
    "    document_types = np.random.choice([\"Invoice\", \"Credit Note\", \"Debit Note\"], num_invoices)\n",
    "    businesses = np.random.choice([\"Electronics\", \"Apparel\", \"Home Goods\", \"Services\"], num_invoices)\n",
    "    payment_terms = np.random.choice([\"Net 30\", \"Net 60\", \"Due on Receipt\", \"15 Days\"], num_invoices)\n",
    "    transaction_cheques = [f\"CHQ-{np.random.randint(10000, 99999)}\" if np.random.rand() > 0.7 else None for _ in range(num_invoices)]\n",
    "    cover_cheques = [f\"CCHQ-{np.random.randint(10000, 99999)}\" if np.random.rand() > 0.8 else None for _ in range(num_invoices)]\n",
    "    overdue = np.random.choice([\"Yes\", \"No\"], num_invoices, p=[0.2, 0.8])\n",
    "    tds = (document_amounts * np.random.uniform(0, 0.05, num_invoices)).round(2)\n",
    "    divisions = np.random.choice([\"Retail\", \"Wholesale\", \"Online\"], num_invoices)\n",
    "    customer_references = [f\"REF-{np.random.randint(1000, 9999)}\" for _ in range(num_invoices)]\n",
    "    groups = np.random.choice([\"Group A\", \"Group B\", \"Group C\"], num_invoices)\n",
    "    sbus = np.random.choice([\"Consumer\", \"Enterprise\"], num_invoices)\n",
    "    sub_sbu_descs = np.random.choice([\"Sub SBU 1\", \"Sub SBU 2\", \"Sub SBU 3\", \"Sub SBU 4\"], num_invoices)\n",
    "    business_descriptions = np.random.choice([\"Sales of Goods\", \"Provision of Services\", \"Subscription Fees\", \"Maintenance Charges\"], num_invoices)\n",
    "    customer_profiles = np.random.choice([\"New\", \"Regular\", \"High Value\"], num_invoices)\n",
    "    payment_methods = np.random.choice([\"Bank Transfer\", \"Credit Card\", \"Cheque\", \"Online Payment\"], num_invoices)\n",
    "    pod_available = np.random.choice([\"Yes\", \"No\"], num_invoices)\n",
    "    texts = np.random.choice([\"Payment Received\", \"Goods Delivered\", \"Service Completed\", None], num_invoices)\n",
    "    ecb = [np.random.choice([\"ECB Applicable\", None]) if np.random.rand() > 0.9 else None for _ in range(num_invoices)]\n",
    "    bill_to_party_codes = [f\"BILL-{np.random.randint(1000, 9999)}\" for _ in range(num_invoices)]\n",
    "    bill_to_party_names = [f\"Bill To Co {np.random.randint(1, 101)}\" for _ in range(num_invoices)]\n",
    "    ship_to_party_codes = [f\"SHIP-{np.random.randint(1000, 9999)}\" for _ in range(num_invoices)]\n",
    "    ship_to_party_names = [f\"Ship To Ltd {np.random.randint(1, 101)}\" for _ in range(num_invoices)]\n",
    "    end_customer_details = np.random.choice([\"End Customer A\", \"End Customer B\", None], num_invoices)\n",
    "    oem_noc = [np.random.choice([\"OEM\", None]) if np.random.rand() > 0.95 else None for _ in range(num_invoices)]\n",
    "    note_attachments = [np.random.choice([\"Attached\", None]) if np.random.rand() > 0.9 else None for _ in range(num_invoices)]\n",
    "    plants = np.random.choice([\"Plant A\", \"Plant B\", \"Plant C\"], num_invoices)\n",
    "    plant_descriptions = np.random.choice([\"Plant A Description\", \"Plant B Description\", \"Plant C Description\"], num_invoices)\n",
    "\n",
    "    # Create the Pandas DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Branch': branches,\n",
    "        'Branch Name': branch_names,\n",
    "        'Customer Code': customer_codes,\n",
    "        'Customer Name Sold T': customer_names,\n",
    "        'Document Type': document_types,\n",
    "        'Document Number': document_numbers,\n",
    "        'Document Date': document_dates,\n",
    "        'Due Date': due_dates,\n",
    "        'Document Amount': document_amounts,\n",
    "        'Outstanding Amount': outstanding_amounts,\n",
    "        'Business': businesses,\n",
    "        'Payment Terms': payment_terms,\n",
    "        'Transaction Cheque': transaction_cheques,\n",
    "        'Cover Cheque': cover_cheques,\n",
    "        'Overdue': overdue,\n",
    "        'TDS': tds,\n",
    "        'Division': divisions,\n",
    "        'Customer Reference': customer_references,\n",
    "        'Group': groups,\n",
    "        'SBU': sbus,\n",
    "        'Sub Sbu Desc': sub_sbu_descs,\n",
    "        'Business Description': business_descriptions,\n",
    "        'Customer Profile': customer_profiles,\n",
    "        'Payment Method': payment_methods,\n",
    "        'POD Available': pod_available,\n",
    "        'Text': texts,\n",
    "        'ECB': ecb,\n",
    "        'Bill To Party Code': bill_to_party_codes,\n",
    "        'Bill To Party Name': bill_to_party_names,\n",
    "        'Ship to Party Code': ship_to_party_codes,\n",
    "        'Ship to Party Name': ship_to_party_names,\n",
    "        'End Customer Details': end_customer_details,\n",
    "        'OEM/NOC': oem_noc,\n",
    "        'Note Attachment': note_attachments,\n",
    "        'Clearing Date': clearing_dates,\n",
    "        'Plant': plants,\n",
    "        'Plant Description': plant_descriptions\n",
    "    })\n",
    "\n",
    "    # Calculate Invoice Ageing (in days from today)\n",
    "    today = date.today()\n",
    "    df['Invoice Ageing(Days)'] = (today - df['Document Date']).dt.days\n",
    "\n",
    "    return df\n",
    "\n",
    "df=sample_s4_data(num_invoices,num_customers)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "#print(df.head())\n",
    "\n",
    "# Display the DataFrame information\n",
    "#df.info()\n",
    "\n",
    "# You can now save this Pandas DataFrame to a CSV file if needed\n",
    "#df.to_csv(\"invoices.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f444078-949e-4bd2-911a-32f77152f8c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25974d27-f938-4b28-a0db-76e532d89620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Specifying a Unity workspace catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d5a4da-ce1b-4370-b325-1c29eb8431ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#%pip install mlflow\n",
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "# If necessary, replace \"main\" and \"default\" with a catalog and schema for which you have the required permissions.\n",
    "CATALOG_NAME = \"xgtp_prod_data\"\n",
    "SCHEMA_NAME = \"sapit-home-prod_challenge_217\"\n",
    "current_user = spark.sql(\"select current_user()\").collect()[0].asDict()[\"current_user()\"].replace(\"@sap.com\", \"\").replace(\".\", \"_\")\n",
    "TABLE_NAME= \"s4_collections_tbl_\"+current_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be1f491f-d42c-475d-b985-dfa3346785bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SPARK DATAFRAME\n",
    "\n",
    "### Pandas dataframe running on a VM/laptop/Mac/windows : \n",
    "\n",
    "- **Eager execution** : When you load data from a CSV file all data is loaded in memory, and this data now has to fit in memory i.e you can only load that much data as fits on the available memory on the machine. \n",
    "- Pandas computations happen on a single core\n",
    "- No query optimizer\n",
    "\n",
    "### **PYSPARK Dataframe on Databricks with Attached Compute cluster**\n",
    "\n",
    "- **Lazy execution** [map operations (like filtering, new column, type conversion etc) are only done when reduce operations (aggregations etc..) are called], pyspark dataframe is immutable i.e once initialized it cannot be changed. Pyspark dataframe is immutable (cannot be changed over time), if a change is requested an internal new dataframe is created to handle the request. This is unlike pandas dataframes and the reason for this is parallelism. In order utlize the full power of parallel distributed computing with pyspark this is the behaviour. \n",
    "Further reading here : https://docs.databricks.com/aws/en/pyspark/  \n",
    "- pyspark (pandas on spark) is scalable to multiple machines in cluster and can process big data.  Even on a single machine it can leverage all cores. \n",
    "- allows spark queries to run on larger than memory datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f411d351-0644-4308-9a1c-48670d2a0f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Spark DataFrame from a pandas DataFrame using Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1502495f-0fa0-4269-a621-339caf224141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
    "s4_inv_collections = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1782e3d1-d406-4413-81a3-491df6bf255a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(s4_inv_collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc78fba6-89e8-4cb6-8611-35a3dc1f1037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create DELTA LAKE TABLE in UNITY CATALOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcf48c7-7c99-4a5a-88ac-af3803582369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-04-29 at 11.12.20 AM.png\" alt=\"Alt Text\" width=\"500\" height=\"600\">\n",
    "\n",
    "# Delta Lake:\n",
    "Databricks Lakehouse architecture works on top of data lake , enhancing it by bringing in reliability, performance, governance by implementing a **transaction layer** on top of the open parquet data format.\n",
    "\n",
    "## Core Architecture\n",
    "- **Storage Format**: Uses Parquet files as the underlying storage format, optimized for efficient data storage and querying\n",
    "- **Transaction Log**: Maintains a transaction log (series of JSON files) that tracks all changes to the table\n",
    "- **Table State**: Current state of a table is determined by replaying the transaction log and applying changes to the base data\n",
    "\n",
    "## Key Features\n",
    "- **ACID Transactions**: Ensures atomicity, consistency, isolation, and durability through transaction log protocol\n",
    "- **Schema Enforcement**: Automatically validates that data being written conforms to the table schema\n",
    "- **Schema Evolution**: Supports adding, dropping, and modifying columns without recreating the table\n",
    "- **Time Travel**: Query data as it existed at a specific point in time using versions or timestamps\n",
    "- **Unified Batch/Streaming**: Supports both batch and streaming workloads with the same data structure\n",
    "\n",
    "## Technical Implementation\n",
    "- **Transaction Protocol**:\n",
    "  - Each transaction creates a new JSON file in the _delta_log directory\n",
    "  - Uses optimistic concurrency control to handle concurrent writers\n",
    "  - Employs snapshot isolation to ensure consistency\n",
    "- **Deletion Vectors**:\n",
    "  - Implements efficient record-level deletes without rewriting files\n",
    "  - Maintains metadata about deleted records through deletion vectors\n",
    "  - Readers filter out deleted records at query time\n",
    "- **Compaction**:\n",
    "  - Combines small files through OPTIMIZE command to improve read performance\n",
    "  - Z-ORDER indexing for efficient data skipping during queries\n",
    "- **Vacuum**: Removes files no longer needed after retention period expires\n",
    "\n",
    "## Data Modifications\n",
    "- **Updates/Deletes**: Records changes in the transaction log without immediately modifying Parquet files\n",
    "- **Merge Operation**: Efficiently combines INSERT, UPDATE, and DELETE operations in a single transaction\n",
    "- **Checkpoint Files**: Periodically creates Parquet-formatted checkpoint files to speed up table state reconstruction\n",
    "\n",
    "## Integration\n",
    "- **Engines**: Works with Spark, Flink, Trino, and other processing engines\n",
    "- **Formats**: Converts seamlessly from other formats (Parquet, CSV, JSON, etc.)\n",
    "- **Cloud Storage**: Optimized for object stores like S3, ADLS, and GCS\n",
    "<br />\n",
    "<br />\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-04-29 at 11.12.35 AM.png\"  width=\"500\" height=\"600\">\n",
    "\n",
    "## Delta Lake Tables in the Context of SAP BDC \n",
    "\n",
    "The shared data from SAP Source systems like S/4HANA, SuccessFactors will be stored in SAP BDC Datalake as a Delta Lake table, this data will then be shared with databricks using **Delta sharing **protocol ensuring **Zero copy of data.** \n",
    "\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-06 at 11.15.46 AM.png\"  width=\"500\" height=\"600\">\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df10b962-5127-4824-927f-c375c390e122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Delta Lake table\n",
    "\n",
    "When we use a CTAS statement ( Create table as ) the default behavior is that a \"DELTA\" enabled table is created and is stored in the data lake storage configured in the Unity catalog. \n",
    "\n",
    "Once the table is created it will be enabled with following features which we can observe from the Unity catalog : \n",
    "\n",
    "- **Time Travel** : This feature allows you to access and query previous versions of your data, providing a full change history and enabling data versioning and auditing.\n",
    "- **DML Operations** : Delta Lake supports standard DML (Data Manipulation Language) operations such as `INSERT`, `UPDATE`, and `DELETE`, allowing you to modify data in a controlled and transactional manner.\n",
    "- **ACID Transactions** : Delta Lake provides Atomicity, Consistency, Isolation, and Durability (ACID) transactions, ensuring that multiple operations are processed as a single, all-or-nothing unit of work, maintaining data consistency and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0b7ba4-183d-402c-9846-335d0dbe3298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the spaces from the column names\n",
    "for c in s4_inv_collections.columns:\n",
    "    s4_inv_collections = s4_inv_collections.withColumnRenamed(c, c.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\"))\n",
    "\n",
    "\n",
    "# Define table names\n",
    "s4_inv_collections_tbl = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "# Write to tables in Unity Catalog\n",
    "#spark.sql(f\"DROP TABLE IF EXISTS `{s4_inv_collections_tbl}`\")\n",
    "#s4_inv_collections.write.mode('overwrite').saveAsTable(f'`{CATALOG_NAME}`.`{SCHEMA_NAME}`.`{TABLE_NAME}`')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde0ec99-4f2d-471c-8d30-ccd20f3a8efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Navigate** to the left hand side panel and open the Unity catalog to see the created table \n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-08 at 10.39.47 AM.png\"  width=\"1000\" height=\"1200\">\n",
    "<br />\n",
    "<br/>\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-08 at 10.57.54 AM.png\"  width=\"1000\" height=\"1200\">\n",
    "<br />\n",
    "\n",
    "Explore the details of the table in the explorer : \n",
    "\n",
    "<br/>\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-08 at 11.00.51 AM.png\"  width=\"1000\" height=\"1200\">\n",
    "<br />\n",
    "\n",
    "Below diagram shows you the structure of the Delta lake table on a datalake, which its various subfolders showing the Transaction log, checkpoint files and deletion vectors etc. \n",
    "<br/>\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-08 at 11.28.18 AM.png\"  width=\"700\" height=\"800\">\n",
    "<br />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865cd45d-412d-48a5-adbc-e0b0721e960a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT CURRENT_METASTORE();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a312114b-5221-46a5-9e3e-f2e3d1790f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data analysis & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbe5571-e880-41a9-ab69-86d3b7344c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md \n",
    "## Explore Data with Summary Stats\n",
    "\n",
    "While using notebooks, you have various options to view summary statistics for dataset. Some of these options are:\n",
    "\n",
    "* using spark DataFrame's built-in method (e.g. `summary()`)\n",
    "* using databricks' utility methods (e.g. `dbutils.data.summarize()`)\n",
    "* using databricks' built-in data profiler/visualizations\n",
    "* using external libraries such as `matplotlib`\n",
    "\n",
    "\n",
    "In this section we will go over the Spark's and Databricks' built-in features for summarizing data. In the next section, we will explore the visualization options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366d6ef3-72a5-4017-bb90-e1762b3c3797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(s4_inv_collections.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1abdc0-8fc5-4295-8beb-56be0d512108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eda1b53-25f4-4d3b-9380-d23d5c3ffaf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s4_inv_collections.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7300b06b-8508-4b18-b671-c54232c7a508",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Strip trailling spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96313f5-47c7-4ecc-b6a0-21156b711195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "columns_to_strip = [\n",
    "    'Document_Number',\n",
    "    'Customer_Code',\n",
    "    'Customer_Name_Sold_T',\n",
    "    'Branch_Name',\n",
    "    'Business',\n",
    "    'Group',\n",
    "    'Plant',\n",
    "    'SBU',\n",
    "    'Division'\n",
    "]\n",
    "\n",
    "for col_name in columns_to_strip:\n",
    "    if col_name in s4_inv_collections.columns:\n",
    "        s4_inv_collections = s4_inv_collections.withColumn(col_name, trim(col(col_name)))\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col_name}' not found in the DataFrame.\")\n",
    "\n",
    "# For example, to show the first few rows with stripped whitespace:\n",
    "display(s4_inv_collections.select('Document_Number',\n",
    "    'Customer_Code',\n",
    "    'Customer_Name_Sold_T',\n",
    "    'Branch_Name',\n",
    "    'Business',\n",
    "    'Group',\n",
    "    'Plant',\n",
    "    'SBU',\n",
    "    'Division').limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f95d91-8c07-4f91-adae-a90e9b13ae76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Value counts\n",
    "Display number of rows for each combination of Business, Group & Business decription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d863ef96-8958-48af-8c3d-983964bcd430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGNvdW50CgojIFNwZWNpZnkgdGhlIGNvbHVtbnMgeW91IHdhbnQgdG8gcGVyZm9ybSB2YWx1ZSBjb3VudHMgb24KY29sdW1uc190b19jb3VudCA9IFsnQnVzaW5lc3MnLCAnR3JvdXAnLCAnQnVzaW5lc3NfRGVzY3JpcHRpb24nXQoKIyBQZXJmb3JtIGdyb3VwQnkoKSBvbiB0aGUgc3BlY2lmaWVkIGNvbHVtbnMgYW5kIHRoZW4gY291bnQgdGhlIG9jY3VycmVuY2VzCnZhbHVlX2NvdW50c19kZiA9IHM0X2ludl9jb2xsZWN0aW9ucy5ncm91cEJ5KCpjb2x1bW5zX3RvX2NvdW50KS5hZ2coY291bnQoIioiKS5hbGlhcygiY291bnQiKSkKCiMgVG8gc2VlIHRoZSByZXN1bHQgaW4gYSB0YWJ1bGFyIGZvcm1hdCAoc2ltaWxhciB0byBQYW5kYXMpLCB5b3UgY2FuIHNob3cgdGhlIERhdGFGcmFtZQpkaXNwbGF5KHZhbHVlX2NvdW50c19kZikK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView685d954\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView685d954\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView685d954\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView685d954) SELECT `Business`,SUM(`count`) `column_34399c99648` FROM q GROUP BY `Business`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView685d954\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "Business",
             "id": "column_34399c99652"
            },
            "y": [
             {
              "column": "count",
              "id": "column_34399c99648",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_34399c99637": {
             "type": "pie",
             "yAxis": 0
            },
            "column_34399c99648": {
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "e68cebeb-c64e-4f2f-90c4-1002ba45c0cb",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 22.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Business",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Business",
           "type": "column"
          },
          {
           "alias": "column_34399c99648",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Specify the columns you want to perform value counts on\n",
    "columns_to_count = ['Business', 'Group', 'Business_Description']\n",
    "\n",
    "# Perform groupBy() on the specified columns and then count the occurrences\n",
    "value_counts_df = s4_inv_collections.groupBy(*columns_to_count).agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# To see the result in a tabular format (similar to Pandas), you can show the DataFrame\n",
    "display(value_counts_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af1908d-7ac4-4bc1-8fc6-ee10d6510793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02c645d-fcb5-4167-bd59-fce1cbddbfa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Filter the DataFrame for 'Group' == 'Group A'\n",
    "filtered_df = s4_inv_collections.filter(col('Group') == 'Group A')\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4cbd5e-a905-4806-abdb-a57f3a1cb798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checking Clearing Date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25996d6e-3836-4b56-b5f7-af8b99887bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "s4_inv_collections.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92a673e-2bc3-45df-8288-e162a3f811a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s4_inv_collections.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a0c416-0a06-499b-8257-841fd7c79077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Import pyspark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9b2b3c-297f-467a-8eac-b02162834d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    avg,\n",
    "    sum ,\n",
    "    count,\n",
    "    stddev,\n",
    "    max ,\n",
    "    month,\n",
    "    quarter,\n",
    "    dayofweek,\n",
    "    when,\n",
    "    lit,\n",
    "    udf,\n",
    "    round,\n",
    "    mean as spark_mean,\n",
    "    transform,\n",
    "    datediff\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd # Import Pandas to help define the cut function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04a89fb-bf85-4a90-9a5d-49bb7d144fbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Feature DAYS_TO_PAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d10e9f7a-dab7-4c2a-b07a-5e2aec20a69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s4_inv_collections = s4_inv_collections.withColumn(\n",
    "    \"Days_to_Pay\", datediff(col(\"Clearing_Date\"), col(\"Document_Date\"))\n",
    ")\n",
    "display(s4_inv_collections.select(\"Clearing_Date\", \"Document_Date\", \"Days_to_Pay\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3073a963-d49c-4a9c-9c29-5096eea7670d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Feature Days_untill_Due"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7ebd25-5b1f-4518-be1e-9c8d3e155851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s4_inv_collections = s4_inv_collections.withColumn(\n",
    "    \"Days_untill_Due\", datediff(col(\"Due_Date\"), col(\"Document_Date\"))\n",
    ")\n",
    "display(s4_inv_collections.select('Due_Date','Document_Date','Days_untill_Due'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba70cea-56b6-4105-8b7a-b0002de4369e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(s4_inv_collections.select('Customer_Code','Customer_Name_Sold_T','Branch_Name','Document_Number','Document_Date','Clearing_Date','Due_Date','Days_untill_Due','Days_to_Pay').head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b196e3-9b23-4728-8234-9279adf48831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Feature Actual_Overdue\n",
    "\n",
    "The overdue column in the dataset shard is not capturing the real overdue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23de7274-c73e-49aa-a148-342f477d8808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s4_inv_collections = s4_inv_collections.withColumn(\n",
    "    \"Actual_Overdue\", \n",
    "    when(col(\"Clearing_Date\") > col(\"Due_Date\"), 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de86f73d-60f6-4a51-8415-6582a35e1ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(s4_inv_collections.select('Customer_Code','Customer_Name_Sold_T','Branch_Name','Document_Number','Document_Date','Clearing_Date','Due_Date','Days_untill_Due','Days_to_Pay','Actual_Overdue').head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3b22ca-14c3-419e-8b67-c1168d76126f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Feature Collection Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d606b1cc-9a51-46df-ad25-b62202a230ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # 3. Calculate collection efficiency features\n",
    "s4_inv_collections = s4_inv_collections.withColumn(\n",
    "    \"collection_ratio\", (col(\"Document_Amount\") - col(\"Outstanding_Amount\"))/col(\"Document_Amount\")\n",
    ")\n",
    "display(s4_inv_collections.select('Document_Amount','Outstanding_Amount','collection_ratio'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24bb7c3-28d8-457f-a0d5-1245979a77be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Customer level aggregated features\n",
    "Here we look at customer specific historical behaviours like :\n",
    "\n",
    "- Actual_Overdue_mean : Mean of invoices overdue\n",
    "- Actual_Overdue_sum : Total number of overdue invoices for the customer\n",
    "- Actual_Overdue_count: Count of historical overdue invoices\n",
    "- Days_to_Pay_mean : historical mean days to pay for the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2ec1fc-c390-4bd3-83cf-c9e5ab1c24e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Create customer-level features\n",
    "\n",
    "\n",
    "customer_features = s4_inv_collections.groupBy('Customer_Code').agg(\n",
    "        avg('Actual_Overdue').alias('Actual_Overdue_mean'),\n",
    "        sum('Actual_Overdue').alias('Actual_Overdue_sum'),\n",
    "        count('Actual_Overdue').alias('Actual_Overdue_count'),\n",
    "        avg('collection_ratio').alias('collection_ratio_mean'),\n",
    "        stddev('collection_ratio').alias('collection_ratio_std'),\n",
    "        max('Days_to_Pay').alias('Days_to_Pay_max'),\n",
    "        avg('Days_to_Pay').alias('Days_to_Pay_mean')\n",
    "        \n",
    "    )\n",
    "#customer_features.write.saveAsTable(f'`{CATALOG_NAME}`.`{SCHEMA_NAME}`.`customer_features`')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb802ea-dd6d-4d37-9754-3d9109d41381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These customer features will also be saved as a delta table in Unity catalog, as it stores important customer historical behaviours which we will also use while running predictions on new data after model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f82fbc-b44e-4085-9f29-59f619c025c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(customer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90574975-cb55-4074-a039-a4c17d2c4772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Time based Features\n",
    "\n",
    "Here we caluclate features line invoice month, invoice day of week, invoice quarter etc to match the billing cycles which customer pay have in their clearing processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca08bfe7-946f-4f84-b356-ee71b579ad39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"CiMgNS4gQ3JlYXRlIHRpbWUtYmFzZWQgZmVhdHVyZXMKIyBGaXJzdCwgZW5zdXJlIERvY3VtZW50IERhdGUgaXMgaW4gZGF0ZSBmb3JtYXQKCiMgQWRkIGludm9pY2VfbW9udGggZmVhdHVyZQpzNF9pbnZfY29sbGVjdGlvbnMgPSBzNF9pbnZfY29sbGVjdGlvbnMud2l0aENvbHVtbigiaW52b2ljZV9tb250aCIsIG1vbnRoKGNvbCgiRG9jdW1lbnRfRGF0ZSIpKSkKCiMgQWRkIGludm9pY2VfcXVhcnRlciBmZWF0dXJlCnM0X2ludl9jb2xsZWN0aW9ucyA9IHM0X2ludl9jb2xsZWN0aW9ucy53aXRoQ29sdW1uKCJpbnZvaWNlX3F1YXJ0ZXIiLCBxdWFydGVyKGNvbCgiRG9jdW1lbnRfRGF0ZSIpKSkKCiMgQWRkIGludm9pY2VfZGF5X29mX3dlZWsgZmVhdHVyZQojIE5vdGU6IEluIFB5U3BhcmssIGRheW9md2VlayByZXR1cm5zIDEgKFN1bmRheSkgdGhyb3VnaCA3IChTYXR1cmRheSkKIyBUbyBtYXRjaCBwYW5kYXMgKHdoZXJlIE1vbmRheT0wLCBTdW5kYXk9NiksIHdlIGNhbiBhZGp1c3Qgd2l0aCBhIGZvcm11bGEKczRfaW52X2NvbGxlY3Rpb25zID0gczRfaW52X2NvbGxlY3Rpb25zLndpdGhDb2x1bW4oImludm9pY2VfZGF5X29mX3dlZWsiLCAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAoZGF5b2Z3ZWVrKGNvbCgiRG9jdW1lbnRfRGF0ZSIpKSArIDUpICUgNykgICAgCiAgICAKZGlzcGxheShzNF9pbnZfY29sbGVjdGlvbnMp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewa7dca60\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewa7dca60\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewa7dca60\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewa7dca60) SELECT `invoice_month`,SUM(`Actual_Overdue`) `column_131cc77f4137`,`Business_Description` FROM q GROUP BY `Business_Description`,`invoice_month`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewa7dca60\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "Business_Description",
             "id": "column_131cc77f4158"
            },
            "x": {
             "column": "invoice_month",
             "id": "column_131cc77f4135"
            },
            "y": [
             {
              "column": "Actual_Overdue",
              "id": "column_131cc77f4137",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_131cc77f4137": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "890b59d7-2e46-4e64-ad72-7f0e32618277",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 81.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "invoice_month",
           "type": "column"
          },
          {
           "column": "Business_Description",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "invoice_month",
           "type": "column"
          },
          {
           "alias": "column_131cc77f4137",
           "args": [
            {
             "column": "Actual_Overdue",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "Business_Description",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 5. Create time-based features\n",
    "# First, ensure Document Date is in date format\n",
    "\n",
    "# Add invoice_month feature\n",
    "s4_inv_collections = s4_inv_collections.withColumn(\"invoice_month\", month(col(\"Document_Date\")))\n",
    "\n",
    "# Add invoice_quarter feature\n",
    "s4_inv_collections = s4_inv_collections.withColumn(\"invoice_quarter\", quarter(col(\"Document_Date\")))\n",
    "\n",
    "# Add invoice_day_of_week feature\n",
    "# Note: In PySpark, dayofweek returns 1 (Sunday) through 7 (Saturday)\n",
    "# To match pandas (where Monday=0, Sunday=6), we can adjust with a formula\n",
    "s4_inv_collections = s4_inv_collections.withColumn(\"invoice_day_of_week\", \n",
    "                                (dayofweek(col(\"Document_Date\")) + 5) % 7)    \n",
    "    \n",
    "display(s4_inv_collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968ae7cb-6104-4989-8d71-4ae41404b0ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Invoice Value Bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b68e5af-f91f-44fd-af45-795c4c205209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, percent_rank, when, lit\n",
    "\n",
    "# Assuming doc_amounts is a column in result_df\n",
    "# If it's a separate list/array, you would first need to incorporate it into your DataFrame\n",
    "\n",
    "# Create a window specification ordered by the document amount column\n",
    "window_spec = Window.orderBy(col(\"Document_Amount\"))\n",
    "\n",
    "# Calculate percent rank (0-1) for each row within the window\n",
    "s4_inv_collections = s4_inv_collections.withColumn(\"percent_rank\", percent_rank().over(window_spec))\n",
    "\n",
    "# Create invoice_value_bracket column based on percentile ranges\n",
    "s4_inv_collections = s4_inv_collections.withColumn(\n",
    "    \"invoice_value_bracket\",\n",
    "    when(col(\"percent_rank\") < 0.2, lit(\"Very Low\"))\n",
    "    .when(col(\"percent_rank\") < 0.4, lit(\"Low\"))\n",
    "    .when(col(\"percent_rank\") < 0.6, lit(\"Medium\"))\n",
    "    .when(col(\"percent_rank\") < 0.8, lit(\"High\"))\n",
    "    .otherwise(lit(\"Very High\"))\n",
    ")\n",
    "\n",
    "# Drop the temporary percent_rank column if you don't need it\n",
    "s4_inv_collections = s4_inv_collections.drop(\"percent_rank\")\n",
    "\n",
    "# Display the result\n",
    "display(s4_inv_collections.select('Customer_Code','Document_Number','Document_Date','Document_Amount','invoice_value_bracket'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae3dba3-1359-4e69-ac63-92f1f24952a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Merge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f7bc93-b144-4ee7-9c83-210c4bf1dbfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"I2ZpbmFsX2RmID0gcGQubWVyZ2UocmVzdWx0X2RmLCBjdXN0b21lcl9mZWF0dXJlcywgb249J0N1c3RvbWVyIENvZGUnLCBob3c9J2xlZnQnKQoKZmluYWxfZGYgPSBzNF9pbnZfY29sbGVjdGlvbnMuam9pbigKICAgIGN1c3RvbWVyX2ZlYXR1cmVzLAogICAgb249IkN1c3RvbWVyX0NvZGUiLAogICAgaG93PSJsZWZ0IgopCmRpc3BsYXkoZmluYWxfZGYp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView266a1f3\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView266a1f3\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView266a1f3\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView266a1f3) SELECT `Business`,SUM(`Document_Amount`) `column_131cc77f4100`,`Group` FROM q GROUP BY `Business`,`Group`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView266a1f3\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "Group",
             "id": "column_131cc77f4104"
            },
            "x": {
             "column": "Business",
             "id": "column_131cc77f4106"
            },
            "y": [
             {
              "column": "Document_Amount",
              "id": "column_131cc77f4100",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_131cc77f4100": {
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "324bd4be-a76c-405c-aac7-08e0501cb999",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 87.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Business",
           "type": "column"
          },
          {
           "column": "Group",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Business",
           "type": "column"
          },
          {
           "alias": "column_131cc77f4100",
           "args": [
            {
             "column": "Document_Amount",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "Group",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#final_df = pd.merge(result_df, customer_features, on='Customer Code', how='left')\n",
    "\n",
    "final_df = s4_inv_collections.join(\n",
    "    customer_features,\n",
    "    on=\"Customer_Code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126e7a7c-a88b-4fdb-9e67-3d99a1e3e529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Late Payment Ratio\n",
    "Out of the total invoices how many were paid late ( mean value of overdue counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf5faca5-9f04-4ea5-a085-48c97ee3f6da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, col\n",
    "# Define a window partitioned by Customer Code\n",
    "window_spec_cc = Window.partitionBy(\"Customer_Code\")\n",
    "\n",
    "# Calculate late_payment_ratio - the mean of Actual_Overdue for each Customer Code\n",
    "final_df = final_df.withColumn(\n",
    "    \"late_payment_ratio\", \n",
    "    mean(col(\"Actual_Overdue\")).over(window_spec_cc)\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f549800f-180c-4c52-a621-2ce0f6e0c4f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Payment Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c44343-6f17-43cb-848c-814dd300b8c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import stddev, col\n",
    "\n",
    "# Define a window partitioned by Customer Code\n",
    "window_spec_cc = Window.partitionBy(\"Customer_Code\")\n",
    "\n",
    "# Calculate payment_std_dev - the standard deviation of Days_to_Pay for each Customer Code\n",
    "final_df = final_df.withColumn(\n",
    "    \"payment_std_dev\", \n",
    "    stddev(col(\"Days_to_Pay\")).over(window_spec_cc)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dafc1f34-7667-4891-9601-5aa1250b5fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Payment Timeliness Score (40 points)\n",
    "\n",
    "For customers with good payment behavior a score close to 40 will be acheived and in other cases it will be lesser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba05452e-0681-4137-bd95-54d194fe8235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGNvbCwgd2hlbiwgZXhwLCBwb3csIGxpdAoKIyBGaXJzdCBjYWxjdWxhdGlvbiAoY29tbWVudGVkIG91dCBpbiBvcmlnaW5hbCBjb2RlKQojIGZpbmFsX2RmID0gZmluYWxfZGYud2l0aENvbHVtbigKIyAgICAgIkludm9pY2VfQW1vdW50X1JhdGlvIiwKIyAgICAgd2hlbihjb2woIkN1c3RvbWVyX0F2Z19JbnZvaWNlX0Ftb3VudCIpICE9IDAsCiMgICAgICAgICAgY29sKCJEb2N1bWVudCBBbW91bnQiKSAvIGNvbCgiQ3VzdG9tZXJfQXZnX0ludm9pY2VfQW1vdW50IikKIyAgICAgKS5vdGhlcndpc2UoMCkKIyApCgojIENhbGN1bGF0ZSB6X3Njb3JlIHdpdGggaGFuZGxpbmcgZm9yIHplcm8gc3RhbmRhcmQgZGV2aWF0aW9uCmZpbmFsX2RmID0gZmluYWxfZGYud2l0aENvbHVtbigKICAgICJ6X3Njb3JlIiwKICAgIHdoZW4oY29sKCJwYXltZW50X3N0ZF9kZXYiKSAhPSAwLAogICAgICAgICAoY29sKCJEYXlzX3RvX1BheV9tZWFuIikgLSBjb2woIkRheXNfdW50aWxsX0R1ZSIpKSAvIGNvbCgicGF5bWVudF9zdGRfZGV2IikKICAgICkub3RoZXJ3aXNlKDApCikKCiMgQ2FsY3VsYXRlIHRpbWVsaW5lc3Nfc2NvcmUKZmluYWxfZGYgPSBmaW5hbF9kZi53aXRoQ29sdW1uKAogICAgInRpbWVsaW5lc3Nfc2NvcmUiLAogICAgd2hlbihjb2woIkRheXNfdG9fUGF5X21lYW4iKSA8PSBjb2woIkRheXNfdW50aWxsX0R1ZSIpLAogICAgICAgICBsaXQoNDApCiAgICApLm90aGVyd2lzZSgKICAgICAgICAgbGl0KDQwKSAqIGV4cCgtcG93KGNvbCgiel9zY29yZSIpLCBsaXQoMS41KSkgLyBsaXQoMikpCiAgICApCikKCiMgRGlzcGxheSB0aGUgcmVzdWx0CmRpc3BsYXkoZmluYWxfZGYp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewc9aa4e3\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewc9aa4e3\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewc9aa4e3\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewc9aa4e3) SELECT `Days_to_Pay`,COUNT(`Document_Number`) `column_131cc77f4044` FROM q GROUP BY `Days_to_Pay`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewc9aa4e3\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Days_to_Pay",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "Days_to_Pay",
             "id": "column_131cc77f4047"
            },
            "y": [
             {
              "column": "Document_Number",
              "id": "column_131cc77f4044",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_131cc77f4044": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b92ff2da-5646-48c3-ad22-3b40f0f6c44f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 113.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Days_to_Pay",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Days_to_Pay",
           "type": "column"
          },
          {
           "alias": "column_131cc77f4044",
           "args": [
            {
             "column": "Document_Number",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGNvbCwgd2hlbiwgZXhwLCBwb3csIGxpdAoKIyBGaXJzdCBjYWxjdWxhdGlvbiAoY29tbWVudGVkIG91dCBpbiBvcmlnaW5hbCBjb2RlKQojIGZpbmFsX2RmID0gZmluYWxfZGYud2l0aENvbHVtbigKIyAgICAgIkludm9pY2VfQW1vdW50X1JhdGlvIiwKIyAgICAgd2hlbihjb2woIkN1c3RvbWVyX0F2Z19JbnZvaWNlX0Ftb3VudCIpICE9IDAsCiMgICAgICAgICAgY29sKCJEb2N1bWVudCBBbW91bnQiKSAvIGNvbCgiQ3VzdG9tZXJfQXZnX0ludm9pY2VfQW1vdW50IikKIyAgICAgKS5vdGhlcndpc2UoMCkKIyApCgojIENhbGN1bGF0ZSB6X3Njb3JlIHdpdGggaGFuZGxpbmcgZm9yIHplcm8gc3RhbmRhcmQgZGV2aWF0aW9uCmZpbmFsX2RmID0gZmluYWxfZGYud2l0aENvbHVtbigKICAgICJ6X3Njb3JlIiwKICAgIHdoZW4oY29sKCJwYXltZW50X3N0ZF9kZXYiKSAhPSAwLAogICAgICAgICAoY29sKCJEYXlzX3RvX1BheV9tZWFuIikgLSBjb2woIkRheXNfdW50aWxsX0R1ZSIpKSAvIGNvbCgicGF5bWVudF9zdGRfZGV2IikKICAgICkub3RoZXJ3aXNlKDApCikKCiMgQ2FsY3VsYXRlIHRpbWVsaW5lc3Nfc2NvcmUKZmluYWxfZGYgPSBmaW5hbF9kZi53aXRoQ29sdW1uKAogICAgInRpbWVsaW5lc3Nfc2NvcmUiLAogICAgd2hlbihjb2woIkRheXNfdG9fUGF5X21lYW4iKSA8PSBjb2woIkRheXNfdW50aWxsX0R1ZSIpLAogICAgICAgICBsaXQoNDApCiAgICApLm90aGVyd2lzZSgKICAgICAgICAgbGl0KDQwKSAqIGV4cCgtcG93KGNvbCgiel9zY29yZSIpLCBsaXQoMS41KSkgLyBsaXQoMikpCiAgICApCikKCiMgRGlzcGxheSB0aGUgcmVzdWx0CmRpc3BsYXkoZmluYWxfZGYp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView015103f\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView015103f\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView015103f\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView015103f) ,min_max AS (SELECT `timeliness_score`,(SELECT MAX(`timeliness_score`) FROM q) `target_column_max`,(SELECT MIN(`timeliness_score`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `timeliness_score`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 10 `step` FROM min_max) SELECT IF(ISNULL(`timeliness_score`),NULL,LEAST(WIDTH_BUCKET(`timeliness_score`,`min_value`,`max_value`,10),10)) `timeliness_score_BIN`,FIRST(`min_value` + ((IF(ISNULL(`timeliness_score`),NULL,LEAST(WIDTH_BUCKET(`timeliness_score`,`min_value`,`max_value`,10),10)) - 1) * `step`)) `timeliness_score_BIN_LOWER_BOUND`,FIRST(`step`) `timeliness_score_BIN_STEP`,COUNT(`timeliness_score`) `COUNT` FROM histogram_meta GROUP BY `timeliness_score_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView015103f\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "timeliness_score",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "timeliness_score",
             "id": "column_131cc77f4064"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_131cc77f4066": {
             "type": "histogram",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "3ee05a85-e068-4dd7-a43e-68145083672c",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 114.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "timeliness_score_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "timeliness_score_BIN",
           "args": [
            {
             "column": "timeliness_score",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "timeliness_score_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "timeliness_score",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "timeliness_score_BIN_STEP",
           "args": [
            {
             "column": "timeliness_score",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "timeliness_score",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, exp, pow, lit\n",
    "\n",
    "# First calculation (commented out in original code)\n",
    "# final_df = final_df.withColumn(\n",
    "#     \"Invoice_Amount_Ratio\",\n",
    "#     when(col(\"Customer_Avg_Invoice_Amount\") != 0,\n",
    "#          col(\"Document Amount\") / col(\"Customer_Avg_Invoice_Amount\")\n",
    "#     ).otherwise(0)\n",
    "# )\n",
    "\n",
    "# Calculate z_score with handling for zero standard deviation\n",
    "final_df = final_df.withColumn(\n",
    "    \"z_score\",\n",
    "    when(col(\"payment_std_dev\") != 0,\n",
    "         (col(\"Days_to_Pay_mean\") - col(\"Days_untill_Due\")) / col(\"payment_std_dev\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Calculate timeliness_score\n",
    "final_df = final_df.withColumn(\n",
    "    \"timeliness_score\",\n",
    "    when(col(\"Days_to_Pay_mean\") <= col(\"Days_untill_Due\"),\n",
    "         lit(40)\n",
    "    ).otherwise(\n",
    "         lit(40) * exp(-pow(col(\"z_score\"), lit(1.5)) / lit(2))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5c6c67-a8fd-470c-bd71-d606cd2c5dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Select Final features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd68dba-5aec-4916-a4cf-2ad99a69e3d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    # Select relevant features for prediction\n",
    "    features = [\n",
    "        'Customer_Code',\n",
    "        'Document_Amount', \n",
    "        'Document_Type',\n",
    "        'invoice_month',\n",
    "        'invoice_value_bracket',\n",
    "        'invoice_day_of_week',\n",
    "        'collection_ratio',\n",
    "        'Actual_Overdue_sum',\n",
    "        'Group',\n",
    "        'SBU',\n",
    "        'Actual_Overdue', \n",
    "        'Days_to_Pay_mean',\n",
    "        'Days_to_Pay_max',\n",
    "        'Days_Past_due',\n",
    "        'Plant',\n",
    "        'Business',\n",
    "        'Division',\n",
    "        'collection_ratio_mean'\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values\n",
    "    #df[features] = df[features].fillna(df[features].mean())\n",
    "    \n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f85e713-85f5-436f-8807-59d81f2f3acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0d18f0-617b-46c8-939d-8c738c827fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_columns = ['Customer Code', 'Document Type', 'Division', 'SBU', 'Payment Terms','Group','Plant','Business','invoice_value_bracket']\n",
    "num_columns = ['Document Amount', 'invoice_month','invoice_day_of_week','invoice_quarter','Actual_Overdue_sum','Actual_Overdue','Days_to_Pay_mean','Days_to_Pay_max','Days_untill_Due','payment_cv','amount_score','late_payment_ratio','payment_std_dev','timeliness_score','Invoice_Amount_Ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8195a3-38e0-4bea-b235-024701a40773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc13cf39-d1ec-46ea-913c-47d7476cfb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd # Import Pandas to help define the cut function.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import StringIndexer  # Import StringIndexer for label encoding\n",
    "from pyspark.ml.regression import GBTRegressor # Import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator # Import RegressionEvaluator\n",
    "import mlflow # Import mlflow\n",
    "import mlflow.spark # Import mlflow.spark\n",
    "\n",
    "num_columns = ['Document_Amount', 'invoice_month','invoice_day_of_week','invoice_quarter','Actual_Overdue_sum','Actual_Overdue','Days_to_Pay_mean','Days_to_Pay_max','Days_untill_Due','late_payment_ratio','payment_std_dev','timeliness_score']\n",
    "num_columns_with_days = num_columns + ['Days_to_Pay']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=num_columns_with_days, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "assembled_df = final_df.select(*num_columns_with_days).dropna() # drop rows with nulls\n",
    "assembled_df = assembler.transform(assembled_df)\n",
    "\n",
    "# 3. Calculate the correlation matrix\n",
    "correlation_matrix = Correlation.corr(assembled_df, \"features\").first()[0].toArray()\n",
    "\n",
    "# 4. Convert the correlation matrix to a Pandas DataFrame for visualization\n",
    "correlation_pd = pd.DataFrame(correlation_matrix, columns=num_columns_with_days, index=num_columns_with_days)\n",
    "\n",
    "# 5. Visualize the correlation matrix using Matplotlib and Seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_pd, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9dc3daa-bbf9-45f5-9b29-63bbabd0f529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_columns = ['Document_Amount', 'invoice_month','invoice_day_of_week','Actual_Overdue_sum','Actual_Overdue','Days_to_Pay_mean','Days_to_Pay_max','Days_untill_Due','late_payment_ratio','payment_std_dev','timeliness_score']\n",
    "num_columns_with_days = num_columns + ['Days_to_Pay']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=num_columns_with_days, outputCol=\"features\")\n",
    "assembled_df = final_df.select(*num_columns_with_days).dropna() # drop rows with nulls\n",
    "assembled_df = assembler.transform(assembled_df)\n",
    "\n",
    "# 3. Calculate the correlation matrix\n",
    "correlation_matrix = Correlation.corr(assembled_df, \"features\").first()[0].toArray()\n",
    "\n",
    "# 4. Convert the correlation matrix to a Pandas DataFrame for visualization\n",
    "correlation_pd = pd.DataFrame(correlation_matrix, columns=num_columns_with_days, index=num_columns_with_days)\n",
    "\n",
    "# 5. Visualize the correlation matrix using Matplotlib and Seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_pd, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc849f43-fe7b-43a4-95e6-732de4ef18a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e71d854-79b4-4633-a636-0711ac83c7f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PREPARED_TABLE=\"s4_training_data\"\n",
    "CATALOG_NAME = \"xgtp_prod_data\"\n",
    "SCHEMA_NAME = \"sapit-home-prod_challenge_217\"\n",
    "current_user = spark.sql(\"select current_user()\").collect()[0].asDict()[\"current_user()\"].replace(\"@sap.com\", \"\").replace(\".\", \"_\")\n",
    "final_df=spark.read.table(f'`{CATALOG_NAME}`.`{SCHEMA_NAME}`.`{PREPARED_TABLE}`')\n",
    "\n",
    "#train=final_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "661819fa-493b-474f-a49d-8fdf56160a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training dataframe with final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa74c12-c52a-4094-b976-09738a545281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df=final_df.select('Customer_Code','Document_Type', 'Division', 'SBU', 'Payment_Terms','Group','Plant','Business','invoice_value_bracket','Document_Amount', 'invoice_month','invoice_day_of_week','Actual_Overdue_sum','Actual_Overdue','Days_to_Pay_mean','Days_to_Pay_max','Days_untill_Due','late_payment_ratio','payment_std_dev','timeliness_score','Days_to_Pay')\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516b8c26-d2e8-4377-8b9b-2426a0685c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## label encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7096f8a0-747a-41f6-b355-64ff3f02e00d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd # Import Pandas to help define the cut function.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import StringIndexer,StandardScaler  # Import StringIndexer for label encoding\n",
    "from pyspark.ml.regression import GBTRegressor # Import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator # Import RegressionEvaluator\n",
    "\n",
    "\n",
    "categorical_columns = ['Customer_Code','Document_Type', 'Division', 'SBU', 'Payment_Terms','Group','Plant','Business','invoice_value_bracket']\n",
    "for col_name in categorical_columns:\n",
    "        indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_encoded\")\n",
    "        model = indexer.fit(train_df)\n",
    "        train_df = model.transform(train_df).drop(col_name) # Drop the original categorical colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656d7c7b-737f-42f6-af1b-47307ab3c176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c610c2d-ad83-465b-97ff-5fe498bae130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training a gradient boosting regressor with ML FLOW\n",
    "\n",
    "In the below code cell we are training a regression model using **ML FLOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69dc28d7-b533-4abe-b587-3ed375438e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Split the data and using standardscaler to standardize numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c61977-b387-4099-86fe-5aec527f11a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Gradient Boosted Regressor with MLflow ---\n",
    "# 1. Select features and target variable\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler, VectorAssembler  \n",
    "from pyspark.ml.regression import GBTRegressor # Import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator # Import RegressionEvaluator\n",
    "import mlflow\n",
    "current_user = spark.sql(\"select current_user()\").collect()[0].asDict()[\"current_user()\"].replace(\"@sap.com\", \"\").replace(\".\", \"_\")\n",
    "\n",
    "feature_columns = ['Customer_Code_encoded','Document_Type_encoded', 'Division_encoded', 'SBU_encoded', 'Payment_Terms_encoded','Group_encoded','Plant_encoded','Business_encoded','invoice_value_bracket_encoded','Document_Amount', 'invoice_month','invoice_day_of_week','Actual_Overdue_sum','Actual_Overdue','Days_to_Pay_mean','Days_to_Pay_max','Days_untill_Due','late_payment_ratio','payment_std_dev','timeliness_score'] # Exclude target\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "model_df = train_df.select(\"Days_to_Pay\", *feature_columns).dropna()\n",
    "model_df = assembler.transform(model_df)\n",
    "\n",
    "    # 2. Split data into training and testing sets\n",
    "(training_data, test_data) = model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # 3. Scale the features using StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(training_data)\n",
    "training_data = scaler_model.transform(training_data)\n",
    "test_data = scaler_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f68401a-73b6-48ed-929b-c138c16915bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4cd97d-cab1-4fd7-a3b1-47cc60ca9035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Define and train the Gradient Boosted Regressor model\n",
    "gbr = GBTRegressor(labelCol=\"Days_to_Pay\", featuresCol=\"scaled_features\") # Use scaled features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f245d3f7-1aeb-4ed7-b95c-013ec2e797d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run a ML FLOW Experiment : RUN 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba93bb6-1d84-4183-a2c1-879d8a652d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "name=f\"{CATALOG_NAME}.{SCHEMA_NAME}.gbt_regression_model_run1\"\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e4321e-e180-4327-965f-c3d4b569031c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start an MLflow experiment (optional, but recommended for organization)\n",
    "with mlflow.start_run(run_name=\"GBT_Regression_Payment_Prediction_RUN1\"+current_user) as run:\n",
    "        # Log parameters (optional)\n",
    "    mlflow.log_param(\"num_trees\", 5)  # Example parameter\n",
    "    mlflow.log_param(\"max_depth\", 2)    # Example parameter\n",
    "\n",
    "        # Train the model\n",
    "    gbr_model = gbr.fit(training_data)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "    predictions = gbr_model.transform(test_data)\n",
    "\n",
    "        # Evaluate the model\n",
    "    evaluator = RegressionEvaluator(labelCol=\"Days_to_Pay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "        # Log the metric\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        # Log the model (crucial for deployment and tracking)\n",
    "    mlflow.spark.log_model(gbr_model, \"gbt_regression_model_run1\")\n",
    "\n",
    "        # Optionally, you can log additional artifacts (e.g., plots, data summaries)\n",
    "\n",
    "    # Register the model in MLflow Model Registry with a specific name\n",
    "    #mlflow.register_model(\n",
    "      #  model_uri=f\"runs:/{run.info.run_id}/gbt_regression_model_run1\",\n",
    "      #  name=f\"{CATALOG_NAME}.{SCHEMA_NAME}.gbt_regression_model_run1\")\n",
    "\n",
    "print(\"Run ID:\", run.info.run_id)\n",
    "print(\"Model registered as: PaymentPredictionGBT_RUN1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e7db31-0c3b-4c16-a411-b15e4500d6e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run a ML FLOW Experiment : RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd714c27-0a50-4660-b075-5d263a5e0957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start an MLflow experiment (optional, but recommended for organization)\n",
    "with mlflow.start_run(run_name=\"GBT_Regression_Payment_Prediction_RUN2\"+current_user) as run:\n",
    "        # Log parameters (optional)\n",
    "    mlflow.log_param(\"num_trees\", 8)  # Example parameter\n",
    "    mlflow.log_param(\"max_depth\", 4)    # Example parameter\n",
    "\n",
    "        # Train the model\n",
    "    gbr_model = gbr.fit(training_data)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "    predictions = gbr_model.transform(test_data)\n",
    "\n",
    "        # Evaluate the model\n",
    "    evaluator = RegressionEvaluator(labelCol=\"Days_to_Pay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "        # Log the metric\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        # Log the model (crucial for deployment and tracking)\n",
    "    mlflow.spark.log_model(gbr_model, \"gbt_regression_model\")\n",
    "\n",
    "        # Optionally, you can log additional artifacts (e.g., plots, data summaries)\n",
    "\n",
    "    # Register the model in MLflow Model Registry with a specific name\n",
    "    #mlflow.register_model(\n",
    "     #   model_uri=f\"runs:/{run.info.run_id}/gbt_regression_model\",\n",
    "     #   name=\"PaymentPredictionGBT_RUN2\")\n",
    "\n",
    "print(\"Run ID:\", run.info.run_id)\n",
    "print(\"Model registered as: PaymentPredictionGBT_RUN2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce23199-9fd1-44c7-850d-3e192e7d4423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Prediction of Target Clearing date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afbfc7ac-e81f-4d61-80b9-049d508a7d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate data for prediciton \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd3559e-da2d-4d2f-bae3-d4faa12c1fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pred=sample_s4_data(100,5)\n",
    "df_pred_spark = spark.createDataFrame(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913a0865-9c6e-4d61-bd5c-fcf94bd3f6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transform the new data to match the features created in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e139ed-675e-46fe-92ed-588a739aeac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = \"xgtp_prod_data\"\n",
    "SCHEMA_NAME = \"sapit-home-prod_challenge_217\"\n",
    "\n",
    "def transform_data_for_pred(df):\n",
    "  for c in df.columns:\n",
    "    df = df.withColumnRenamed(c, c.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\"))\n",
    "  df = df.withColumn(\"Days_untill_Due\", datediff(col(\"Due_Date\"), col(\"Document_Date\")))\n",
    "  df = df.withColumn(\"Actual_Overdue\", lit(0))\n",
    "  df = df.withColumn(\"collection_ratio\", col(\"Document_Amount\")/col(\"Outstanding_Amount\"))\n",
    "  df = df.withColumn(\"invoice_month\", month(col(\"Document_Date\")))\n",
    "  df = df.withColumn(\"invoice_quarter\", quarter(col(\"Document_Date\")))\n",
    "  df = df.withColumn(\"invoice_day_of_week\", dayofweek(col(\"Document_Date\")))  \n",
    "  customer_features=spark.read.table(f'`{CATALOG_NAME}`.`{SCHEMA_NAME}`.`customer_features`')\n",
    "  df = df.join(customer_features, on='Customer_Code', how='left')\n",
    "  df=df.withColumn(\"late_payment_ratio\",col(\"Actual_Overdue_mean\"))\n",
    "  df=df.withColumn(\"timeliness_score\",lit(15.09))\n",
    "  df=df.withColumn(\"payment_std_dev\",lit(15.09))\n",
    "  #generate invoice value brackets using the same window spec as created before training\n",
    "  df = df.withColumn(\"percent_rank\", percent_rank().over(window_spec))\n",
    "  df = df.withColumn(\n",
    "    \"invoice_value_bracket\",\n",
    "    when(col(\"percent_rank\") < 0.2, lit(\"Very Low\"))\n",
    "    .when(col(\"percent_rank\") < 0.4, lit(\"Low\"))\n",
    "    .when(col(\"percent_rank\") < 0.6, lit(\"Medium\"))\n",
    "    .when(col(\"percent_rank\") < 0.8, lit(\"High\"))\n",
    "    .otherwise(lit(\"Very High\"))\n",
    "  )\n",
    "  df = df.drop(\"percent_rank\")\n",
    "  #encode categorical columns\n",
    "  categorical_columns = ['Customer_Code','Document_Type', 'Division', 'SBU', 'Payment_Terms','Group','Plant','Business','invoice_value_bracket']\n",
    "  for col_name in categorical_columns:\n",
    "        indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_encoded\")\n",
    "        model = indexer.fit(df)\n",
    "        df = model.transform(df).drop(col_name)\n",
    "  #apply scalling \n",
    "  feature_columns = ['Customer_Code_encoded','Document_Type_encoded', 'Division_encoded', 'SBU_encoded', 'Payment_Terms_encoded','Group_encoded','Plant_encoded','Business_encoded','invoice_value_bracket_encoded','Document_Amount', 'invoice_month','invoice_day_of_week','Actual_Overdue_sum','Actual_Overdue','Days_to_Pay_mean','Days_to_Pay_max','Days_untill_Due','late_payment_ratio','payment_std_dev','timeliness_score'] # Exclude target\n",
    "  assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "  model_df1 = df.select(*feature_columns).dropna()\n",
    "  model_df1 = assembler.transform(df)\n",
    "\n",
    "    # 2. Split data into training and testing sets\n",
    "  #(training_data, test_data) = model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # 3. Scale the features using StandardScaler\n",
    "  scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "  scaler_model = scaler.fit(model_df1)\n",
    "  model_df1 = scaler_model.transform(model_df1)\n",
    "\n",
    "  return model_df1\n",
    "transformed_df_pred=transform_data_for_pred(df_pred_spark)\n",
    "display(transformed_df_pred.select(\"features\",\"scaled_features\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba949496-0b89-440f-82ce-c4a6566cda3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Predicted days to Pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a43f449-5b18-4054-ba88-c4a03be71b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = gbr_model.transform(transformed_df_pred)\n",
    "display(predictions.select(\"prediction\", *feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7436416-65a4-4cd4-84c5-565641c48e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48eb0a7e-9aef-43d1-849f-f05050ec1ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Predicted date of Payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da33c75d-5d38-40d0-a877-adb8245b4368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "predictions = predictions.withColumn(\"Predicted_Days_to_Pay\", col(\"prediction\").cast(IntegerType()))\n",
    "predictions = predictions.withColumn(\"Predicted_Date_of_Payment\", col(\"Document_Date\")+col(\"prediction\").cast(IntegerType()))\n",
    "display(predictions.select(\"Predicted_Days_to_Pay\",\"Predicted_Date_of_Payment\",\"Document_Date\",\"Due_Date\",\"Document_Number\",\"Customer_Code_encoded\",\"Document_Amount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee89e4f-814a-4cfc-919a-3014d516cc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Track your ML Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3562af93-8759-4dae-a287-c9508fb54088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can either click on the **experiments** link after the output on the model training cell or follow the right navigation panel to open the ML Flow Experiments section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31231770-b270-4611-9643-296e28d44169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br/>\n",
    "<img src=\"BDC_Learn2Win_Images/Screenshot 2025-05-14 at 5.56.03 PM.png\"  width=\"500\" height=\"600\">\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c32dbcc-0b8c-495f-9548-f140f6752a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "mlflow"
    ],
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 507299458057595,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) BDC_Databricks_Handson_L2W_H1_2025",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
